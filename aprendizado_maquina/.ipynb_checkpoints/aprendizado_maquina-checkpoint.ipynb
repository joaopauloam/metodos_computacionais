{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizado de Máquina\n",
    "\n",
    "+ Aprendizado de Máquina é uma área de IA cujo objetivo é o desenvolvimento de técnicas computacionais sobre o aprendizado bem como a construção de sistemas capazes de adquirir conhecimento de forma automática\n",
    "+ Um sistema de aprendizado é um programa de computador que toma decisões baseado em experiências acumuladas através da solução bem sucedida de problemas anteriores\n",
    "+ Nas últimas décadas o aprendizado de máquina tornou-se um dos pilares em tecnologia da informação\n",
    "+ Embora hoje seja parte central do nosso cotidiano, seus conceitos não são claros para a maioria da pessoas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplos\n",
    "\n",
    "+ Ranqueamento de páginas web\n",
    "    + Em uma busca na web o motor de busca retorna os resultados mais relevantes\n",
    "    + Como classificar a relevância das páginas encontradas?\n",
    "![alt_text](ranqueamento.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplos\n",
    "\n",
    "+ Filtro colaborativo\n",
    "    + Livrarias como a Amazon ou serviços de streaming como o Netflix usam essa informação para sugerir livros, filmes ou séries\n",
    "    + Essa sugestão pode ser \"aprendida\" com base em compras passadas do próprio usuário ou de decisões tomadas por usuários parecidos\n",
    "![alt_text](filtro_colaborativo.png)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplos\n",
    "\n",
    "+ Tradução automática\n",
    "    + Documentos traduzidos podem servir de exemplo para a aprendizagem de um tradutor automático\n",
    "+ Reconhecimento de discurso\n",
    "    + Traduzir um áudio em texto\n",
    "    + Traduzir um texto escrito a mão em um texto digitado    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplos\n",
    "\n",
    "+ Classificador\n",
    "    + Dada a foto de uma pessoa um sistema de segurança pode tentar identificar em sua base de dados quem é aquela pessoa\n",
    "    + Ou ainda, o sistema pode verificar se aquela pessoa é quem realmente diz ser\n",
    "![alt_text](verificacao.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![O dilema das fritas enroladas](video.png)](https://www.ted.com/talks/jennifer_golbeck_the_curly_fry_conundrum_why_social_media_likes_say_more_than_you_might_think?language=pt-br#t-1752 \"O dilema das fritas enroladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classificação\n",
    "\n",
    "+ Problema comum em aprendizado de máquina\n",
    "+ Exemplo: filtro de spam\n",
    "    + Depende do usuário a classificação de um e-mail em spam ou não\n",
    "+ Exemplo: Diagnosticar câncer em um paciente com base em dados histológicos\n",
    "+ Estes são exemplos de classificação binária\n",
    "![alt_text](classificacao_binaria.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dados\n",
    "\n",
    "+ É útil caracterizar os problemas de aprendizagem de acordo com o tipo de dados que eles trabalham\n",
    "+ Isso possibilita que quando novos desfios são encontrados com tipos de dados similares, técnicas similares possam ser utilizadas\n",
    "+ Por exemplo, processamento de linguagem natural e bioinformática usam técnicas bastante similares para tratarem as strings enconytradas nos textos de linguagem natural e nas sequências de DNA\n",
    "+ Os **vetores** constituem a entidade básica para codificar os dados trabalhados em aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dados\n",
    "\n",
    "+ Por exemplo, para uma companhia de seguro de vida seria interessante obter um vetor de variáveis  (pressão sanguínea, batimentos cardíacos, altura, peso, nível de colesterol, fumante, gênero) para inferir a expectativa de vida de um cliente em potencial.\n",
    "+ Um engenheiro pode querer descobrir dependências entre pares (voltagem, corrente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dados\n",
    "\n",
    "+ Referência: http://dcm.ffclrp.usp.br/~augusto/teaching/ami/AM-I-Conceitos-Definicoes.pdf\n",
    "+ De maneira geral, dados pares (x,f(x)), inferir f.\n",
    "![alt_text](dados.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dados\n",
    "\n",
    "+ Dada uma amostra finita, normalmente é impossível determinar a verdadeira função f.\n",
    "+ Abordagem: Encontre uma hipótese ( modelo ) nos exemplos de treinamento e assuma que a hipótese se repita para exemplos futuros também"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exemplos\n",
    "\n",
    "+ [Chemistry world](https://www.chemistryworld.com/1616.tag)\n",
    "    + [Ligand selector steers C–N cross-couplings down most sustainable path](Ligand.pdf)\n",
    "    + [Neural network folds proteins a million times faster than its competitors](Neural.pdf)\n",
    "    + [Algorithm accurately predicts mechanical properties of existing and theoretical MOFs](MOF.pdf)\n",
    "    + [Lithium–ion battery book written by machine learning algorithm](Lithium.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dados\n",
    "\n",
    "![alt_text](dados2.png)\n",
    "+ $f: X_1 \\times X_2 \\times X_3 \\times X_4 \\rightarrow Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problemas\n",
    "\n",
    "+ Classificação binária\n",
    "+ Classificação multiclasse\n",
    "+ Estimativa estruturada\n",
    "+ Regressão\n",
    "+ Detecção de novidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarquia do aprendizado\n",
    "![alt_text](hierarquia_aprendizado.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Como funciona o aprendizado de máquina\n",
    "![alt_text](aprendizado.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Como funciona o aprendizado de máquina\n",
    "![alt_text](aprendizado2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Como funciona o aprendizado de máquina\n",
    "![alt_text](aprendizado3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Como funciona o aprendizado de máquina\n",
    "![alt_text](aprendizado4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AS X AnS\n",
    "+ Aprendizado Supervisionado\n",
    "    + Compreender o relacionamento entre os atributos e a classe\n",
    "    + Predizer a classe de novos exemplos o melhor possível\n",
    "+ Aprendizado Não Supervisionado\n",
    "    + Encontrar representações úteis dos exemplos, tais como:\n",
    "        + Encontrar agrupamentos (clusters)\n",
    "        + Redução da dimensão\n",
    "        + Encontrar as causas ou as fontes ocultas dos exemplos\n",
    "        + Modelar a densidade dos exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo, Atributo e Classe\n",
    "+ Exemplo\n",
    "    + Exemplo, caso, registro ou instância\n",
    "    + É um conjunto fixo de atributos\n",
    "    + Um exemplo descreve o objeto de interesse, tal como um paciente, exemplos médicos sobre uma determinada doença ou histórico de clientes de uma dada companhia\n",
    "+ Atributo\n",
    "    + Atributo, campo ou característica (feature)\n",
    "    + Uma única característica de um exemplo\n",
    "+ Classe\n",
    "    + Atributo especial que descreve o fenômeno de interesse (somente no Aprendizado Supervisionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo, Atributo e Classe\n",
    "![alt_text](exemplos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conjuntos de Exemplos\n",
    "\n",
    "+ Em geral, um conjunto de exemplos é dividido em dois subconjuntos disjuntos:\n",
    "    + conjunto de treinamento que é usado para o aprendizado do conceito e o\n",
    "    + conjunto de teste que é usado para medir o grau de efetividade do conceito aprendido\n",
    "+     Os subconjuntos são disjuntos para assegurar que as medidas obtidas utilizando o conjunto de teste sejam de um conjunto diferente do usado para realizar o aprendizado, tornando a medida estatisticamente válida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Erro aparente\n",
    "![alt_text](erro1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Erro verdadeiro\n",
    "![alt_text](erro2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparação de dados\n",
    "\n",
    "+ Fase que antecede o processo de aprendizagem, para facilitar ou melhorar o processo.\n",
    "+ Exemplos:\n",
    "    + remover exemplos incorretos\n",
    "    + transformar o formato dos exemplos para que possam ser usados com um determinado indutor\n",
    "    + selecionar um subconjunto de atributos relevantes (FSS – Feature Subset Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sobreajuste (overfitting)\n",
    "\n",
    "+ A hipótese extraída a partir dos exemplos é muito específica para o conjunto de treinamento\n",
    "+ A hipótese apresenta um bom desempenho para o conjunto de treinamento, mas um desempenho ruim para os casos fora desse conjunto\n",
    "![alt_text](overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Subajuste (underfitting)\n",
    "\n",
    "+ A hipótese induzida apresenta um desempenho ruim tanto no conjunto de treinamento como no de teste\n",
    "![alt_text](overfitting_underfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dado, Informação, Conhecimento\n",
    "\n",
    "+ Dado: é a estrutura fundamental sobre a qual um sistema de informação é construído\n",
    "+ Informação: a transformação de dados em informação é freqüentemente realizada através da apresentação dos dados em uma forma compreensível ao usuário\n",
    "+ Conhecimento:\n",
    "    + Fornece a capacidade de resolver problemas, inovar e aprender baseado em experiências prévias\n",
    "    + Uma combinação de instintos, idéias, regras e procedimentos que guiam as ações e decisões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estrutura\n",
    "![alt_text](estrutura.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importante\n",
    "\n",
    "+ Dado não é Informação\n",
    "+ Informação não é Conhecimento\n",
    "+ Conhecimento não é Inteligência\n",
    "+ Inteligência não é Sabedoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quimiometria\n",
    "\n",
    "+ Talvez a principal aplicação de aprendizado de máquina em química se dê no campo de pesquisa conhecido como **quimiometria**.\n",
    "+ Algumas definições de quimiometria:\n",
    "    + A quimiometria é a aplicação de ferramentas matemáticas e estatísticas à química (Bruce Kowalski).\n",
    "    + Quimiometria é a aplicação de estatística  à análise de dados químicos (de química orgânica, analítica ou de química medicinal) e o planejamento de experimentos químicos e simulações (IUPAC).\n",
    "    + Quimiometria é a disciplina química que usa métodos matemáticos e estatísticos para:\n",
    "        + planejar ou otimizar procedimentos experimentais; e\n",
    "        + extrair o máximo da informação química relevante, através da análise dos dados (Bruce Kowalski).\n",
    "    + Quimiometria é o que os quimiometristas fazem (Quimiometrista anônimo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quimiometria\n",
    "\n",
    "+ Na parte de análise de dados os principais métodos usados em quimiometria são:\n",
    "    + Análise exploratória (aprendizado não supervisionado)\n",
    "        + HCA\n",
    "        + PCA\n",
    "    + Métodos de regressão (aprendizado supervisionado)\n",
    "        + MLR\n",
    "        + PCR\n",
    "        + PLS\n",
    "    + Métodos de classificação (aprendizado supervisionado)\n",
    "        + KNN\n",
    "        + SIMCA\n",
    "        + PLS-DA\n",
    "        + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn\n",
    "https://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise de Agrupamentos por métodos hierárquicos (HCA)\n",
    "\n",
    "+ A nálise de agrupamentos por métodos hierárquicos (HCA, do inglês *hierarquical cluster analysis*), teve origem na taxonomia numérica\n",
    "+ É um método de aprendizado não-supervisionado de reconhecimento de padrões, adequado para descobrir \"padrões naturais\" de comportamento entre amostras\n",
    "+ HCA é útil para reduzir a dimensionalidade dos dados de grande porte, por exemplo, permitindo que dezenas de milhares de genes possam ser representados por poucos grupos de genes de comportamento semelhante\n",
    "+ Outro exemplo uso do HCA é na detecção de amostras com comportamento anômalo em um conjunto de dados\n",
    "+ Outro exemplo na busca por estruturas cristalográficas similares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise de Agrupamentos por métodos hierárquicos (HCA)\n",
    "\n",
    "+ O HCA é uma técnica interessante porque permite a visualização de dados multidimensionais por meio de um esquema bidimensional\n",
    "+ Os resultados são apresentados na forma de uma árvore hierárquica também conhecida como **dendrograma**\n",
    "+ O HCA é uma técnica algomerativa, na qual cada objeto é considerado um grupo unitário. A seguir eles vão sendo agrupados sistematicamente por ordem de similaridade, em um processo iterativo, até que todos eles formem um único grupo\n",
    "+ É muito razoável assumir que amostras próximas entre si no espaço multidimensional, $R^J$, sejam semelhantes em relação às variáveis selecionadas\n",
    "+ Isso quer dizer que uma maneira de determinar o quanto um objeto é semelhante a outro é por meio do cálculo da distância entre eles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise de Agrupamentos por métodos hierárquicos (HCA)\n",
    "\n",
    "+ Essa distância pode ser:\n",
    "    + Euclideana\n",
    "        $d_{AB}^M = [(\\mathbf{x_A}-\\mathbf{x_B})^T (\\mathbf{x_A}-\\mathbf{x_B})]^{1/2} = \\|\\mathbf{x_A}-\\mathbf{x_B} \\|_2$\n",
    "    + Manhattan\n",
    "        $d_{AB}^M = \\|\\mathbf{x_A}-\\mathbf{x_B} \\|_1 = \\sum_{j=1}^{J}|x_{aj}-x_{bj}|$\n",
    "    + Mahalanobis\n",
    "        $d_{AB}^M = [(\\mathbf{x_A}-\\mathbf{x_B})^T \\mathbf{Var}^{-1}(\\mathbf{x_A}-\\mathbf{x_B})]^{1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"liquens.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_numbers = df.iloc[2:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_numbers.index = range(1,len(df_numbers)+1)\n",
    "df_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range (df_numbers.shape[0]):\n",
    "    for j in range (df_numbers.shape[1]):\n",
    "        if df_numbers.iloc[i,j] == \"<LD\":\n",
    "            df_numbers.iloc[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Z = linkage(df_numbers, 'ward')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z,orientation=\"right\",labels=[(df_numbers.index[i],df.iloc[2+i,1],df.iloc[2+i,0]) for i in range(len(df_numbers))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise de componentes principais (PCA)\n",
    "\n",
    "+ A análise de componentes principais (PCA, do inglês *principal component analysis*) é um método utilizado para projetar os dados multivariados em um espaço de menor dimensão reduzindo, assim, a dimensionalidade do espaço original do conjunto de dados, sem que a relação entre as amostras seja afetada\n",
    "+ Utilizando essa metodologia é possível descobrir, visualizar e interpretar as diferenças existentes entre as variáveis e examinar as relações que podem existir entre as amostras\n",
    "+ Esa análise também nos permite detectar amostras que apresentam um comportamento distinto (atípico)\n",
    "+ Os metodos de projeção são importantes na área de meteorologia para o tratamento de imagens de satélite\n",
    "+ Em química são importantes para o tratamento de dados de espectroscopia e cromatografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise de componentes principais (PCA)\n",
    "\n",
    "+ Em PCA é possível fazer uma \"compressão\" dos dados por meio da criação de um novo conjunto de variáveis que são combinações lineares das variáveis originais\n",
    "+ Essas novas variáveis são as componentes principais, PC, também conhecidas por fatores ou autovetores\n",
    "+ Um propriedades importante das PCs é que são não correlacionadas e ortogonais entre si\n",
    "+ Outra propriedade importante é que a primeira PC (PC1) é definida pela direção que descreve a máxima variância dos dados originais\n",
    "+ PC2 tem a direção de máxima variância dos dados no subespaço ortogonal à PC1 e assim sucessivamente\n",
    "+ O número de componentes principais, *A*, que é necessário para descrever adequadamente o sistema em estudo, é chamado de dimensão intrínseca, pseudoposto ou posto químico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise de componentes principais (PCA)\n",
    "\n",
    "+ Matematicamente PCA equivale a decompor a matriz  **X**(*I* X *J*) em duas matrizes, uma de escores **T** e uma matriz ortonormal de pesos (*loadings*) **L**, de tal maneira que\n",
    "\n",
    "$\\mathbf{X} = \\mathbf{T}\\mathbf{L}^T$\n",
    "\n",
    "+ em que os escores expressam as relações entre as amostras, enquanto que os pesos indicam as relações entre as variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "df = pd.read_excel(\"liquens.xls\")\n",
    "df_numbers = df.iloc[2:,2:]\n",
    "df_numbers = df_numbers.replace(\"<LD\",0)\n",
    "Xa = StandardScaler().fit_transform(df_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)\n",
    "pca.fit(Xa)  \n",
    "print(pca.explained_variance_ratio_)  \n",
    "print(pca.singular_values_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "L= pca.components_.T\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "T = pca.transform(Xa)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import LabelSet, Label, ColumnDataSource\n",
    "import numpy as np\n",
    "source = ColumnDataSource(data=dict(PC1=T[:,0],\n",
    "                                    PC2=T[:,1],\n",
    "                                    names=df_numbers.index))\n",
    "\n",
    "output_notebook()\n",
    "p = figure(title=\"Gráfico de escores\", x_axis_label='PC1', y_axis_label='PC2')\n",
    "p.scatter('PC1','PC2',color='blue',size=6, source=source)\n",
    "labels = LabelSet(x='PC1', y='PC2', text='names', level='glyph',\n",
    "              x_offset=5, y_offset=5, source=source, render_mode='canvas')    \n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "source = ColumnDataSource(data=dict(PC1=L[:,0],\n",
    "                                    PC2=L[:,1],\n",
    "                                    names=df_numbers.columns))\n",
    "\n",
    "output_notebook()\n",
    "p = figure(title=\"Gráfico de pesos(loadings)\", x_axis_label='PC1', y_axis_label='PC2')\n",
    "p.scatter('PC1','PC2',color='blue',size=6, source=source)\n",
    "labels = LabelSet(x='PC1', y='PC2', text='names', level='glyph',\n",
    "              x_offset=5, y_offset=5, source=source, render_mode='canvas')    \n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Novo conjunto de dados\n",
    "flav = pd.read_csv(\"flavonoides.csv\",sep='\\t',index_col=0)\n",
    "flav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = flav.iloc[:,:-1]\n",
    "y = flav.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## PCA\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import LabelSet, Label, ColumnDataSource\n",
    "import numpy as np\n",
    "pca = PCA(n_components=6)\n",
    "Xa = StandardScaler().fit_transform(X)\n",
    "pca.fit(Xa)\n",
    "#T = Xa.dot(pca.components_.T)\n",
    "T = pca.fit_transform(Xa)\n",
    "source = ColumnDataSource(data=dict(PC1=T[:,0],\n",
    "                                    PC2=T[:,1],\n",
    "                                    names=flav.index))\n",
    "\n",
    "output_notebook()\n",
    "p = figure(title=\"Gráfico de escores\", x_axis_label='PC1', y_axis_label='PC2')\n",
    "p.scatter('PC1','PC2',color='blue',size=6, source=source)\n",
    "labels = LabelSet(x='PC1', y='PC2', text='names', level='glyph',\n",
    "              x_offset=5, y_offset=5, source=source, render_mode='canvas')    \n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "L = pca.components_.T\n",
    "source = ColumnDataSource(data=dict(PC1=L[:,0],\n",
    "                                    PC2=L[:,1],\n",
    "                                    names=flav.columns[:-1]))\n",
    "\n",
    "output_notebook()\n",
    "p = figure(title=\"Gráfico de pesos(loadings)\", x_axis_label='PC1', y_axis_label='PC2')\n",
    "# for i in range(np.shape(T)[0]):\n",
    "#     x = T[i,0]\n",
    "#     y = T[i,1]\n",
    "#     p.circle(x,y,color='blue')\n",
    "    #p.text(x+0.3,y+0.3,flav.index[i])\n",
    "p.scatter('PC1','PC2',color='blue',size=6, source=source)\n",
    "labels = LabelSet(x='PC1', y='PC2', text='names', level='glyph',\n",
    "              x_offset=5, y_offset=5, source=source, render_mode='canvas')    \n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# HCA\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Z = linkage(Xa,'ward')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z,orientation=\"right\", labels=flav.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos de regressão\n",
    "\n",
    "+ Um modelo de regressão é obtido por meio da resolução do sistema linear\n",
    "\n",
    "$\\mathbf{Xb} = \\mathbf{y}$\n",
    "\n",
    "+ Solução\n",
    "    + $\\mathbf{X^{+}Xb} = \\mathbf{X^{+}y}$\n",
    "    + $\\mathbf{b} = \\mathbf{X^{+}y}$\n",
    "+ Em que $\\mathbf{X^{+}}$ é a inversa generalizada de Moore-Penrose    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos de regressão\n",
    "\n",
    "+ A resolução do sistema inclui as seguintes soluções:\n",
    "    1. $\\mathbf{X^{+}} = \\mathbf{X^{-1}}$ quando $\\mathbf{X}$ é quadrada e inversível\n",
    "    2. $\\Arrowvert {\\mathbf{y-Xb}} \\Arrowvert^2_{2} = min$ Solução de quadrados mínimos\n",
    "+ Normalmente essa equação linear é obtida através de um dos seguintes métodos:\n",
    "    + MLR\n",
    "    + PCR\n",
    "    + PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLR (do inglês, Muliple Linear Regression)\n",
    "\n",
    "+ Nada mais é que a resolução direta do problema de quadrados mínimos\n",
    "+ A matriz $\\mathbf{X}$ deve ter mais linhas (amostras) que colunas (variáveis)\n",
    "+ As colunas (variáveis) não podem ser correlacionadas, pois assim o sistema não terá uma solução estável"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métodos de projeção\n",
    "\n",
    "+ Devido às limitações impostas pelo método MLR, uma solução é projetar as variáveis da matriz $\\mathbf{X}$ em um espaço de dimensão menor\n",
    "+ Se $\\mathbf{X}$ é uma matriz ($I \\times J$) de posto $P$, ela pode ser decomposta em\n",
    "\n",
    "$\\mathbf{X} = \\mathbf{URW^{t}}$\n",
    "\n",
    "+ $\\mathbf{U}$ é ortonormal de dimensões ($I \\times P$)\n",
    "+ $\\mathbf{W}$ é ortonormal de dimensões ($J \\times P$)\n",
    "+ $\\mathbf{R}$ é não singular de dimensões ($P \\times P$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métodos de projeção\n",
    "\n",
    "+ Assim temos\n",
    "    + $\\mathbf{R} = \\mathbf{U^{t}XW}$\n",
    "    + $\\mathbf{X^{+}} = \\mathbf{WR^{-1}U^{t}}$\n",
    "+ e, portanto\n",
    "    + $\\mathbf{b} = \\mathbf{WR^{-1}U^{t}y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métodos de projeção\n",
    "\n",
    "+ Essa formulação é geral e cobre uma infinidade de métodos, dentre os quais destacam-se:\n",
    "    + PCR (A matriz R é diagonal)\n",
    "    + PLS (A matriz R é bidiagonal)\n",
    "+ O método PLS costuma ser mais usado, pois ele projeta as variáveis originais em um espaço de menor dimensão formado pelas variáveis latentes levando em consideração a informação contida na variável dependente   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Carregando novos dados\n",
    "X = pd.read_csv(\"Xdat.txt\",sep='\\t',index_col=0)\n",
    "print(\"Dimensões de X\")\n",
    "print(X.shape)\n",
    "y = pd.read_csv(\"pIC50dat.txt\",sep='\\t')\n",
    "print(\"Dimensões de y\")\n",
    "print(y.shape)\n",
    "\n",
    "# Autoescalando os dados\n",
    "scaler = StandardScaler().fit(X)\n",
    "print(\"Médias de X\")\n",
    "print(scaler.mean_)\n",
    "print(\"Varâncias de X\")\n",
    "print(scaler.var_)\n",
    "# Autoescalando de fato\n",
    "Xa = scaler.transform(X)\n",
    "print(\"Médias de X autoescalado\")\n",
    "print(Xa.mean(axis=0))\n",
    "print(\"Desvios padrão de X autoescalado\")\n",
    "print(Xa.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# MLR\n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mlr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit\n",
    "mlr.fit(Xa, y)\n",
    " \n",
    "# # Calibration\n",
    "y_c = mlr.predict(Xa)\n",
    "y_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cv = cross_val_predict(mlr, Xa, y, cv=len(Xa))\n",
    "y_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "score_c = r2_score(y, y_c)\n",
    "print(score_c)\n",
    "score_cv = r2_score(y, y_cv)\n",
    "print(score_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "X_new = SelectKBest(f_regression, k=6).fit_transform(Xa, y)\n",
    "np.shape(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "mlr.fit(X_new, y)\n",
    " \n",
    "# # Calibration\n",
    "y_c = mlr.predict(X_new)\n",
    "\n",
    "\n",
    "score_c = r2_score(y, y_c)\n",
    "print(score_c)\n",
    "y_cv = cross_val_predict(mlr, X_new, y, cv=len(y))\n",
    "score_cv = r2_score(y, y_cv)\n",
    "print(score_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# PCR\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 8)\n",
    "pca.fit(Xa)\n",
    "T = pca.fit_transform(Xa)\n",
    "\n",
    "pcr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit\n",
    "pcr.fit(T, y)\n",
    "\n",
    "# Calibration\n",
    "y_c_pcr = pcr.predict(T)\n",
    "\n",
    "# Cross-validation\n",
    "y_cv_pcr = cross_val_predict(pcr, T, y, cv=len(y))\n",
    "\n",
    "# Calculate scores for calibration and cross-validation\n",
    "score_c_pcr = r2_score(y, y_c_pcr)\n",
    "print(score_c_pcr)\n",
    "score_cv_pcr = r2_score(y, y_cv_pcr)\n",
    "print(score_cv_pcr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "c_scores = []\n",
    "mse_c = []\n",
    "mse_cv = []\n",
    "for i in range(8):\n",
    "    Tp = T[:,:i+1]\n",
    "    # Calibration\n",
    "    pcr.fit(Tp,y)\n",
    "    y_c_pcr = pcr.predict(Tp)\n",
    "\n",
    "    # Cross-validation\n",
    "    y_cv_pcr = cross_val_predict(pcr, Tp, y, cv=len(Xa))\n",
    "\n",
    "    # Calculate scores for calibration and cross-validation\n",
    "    c_scores.append(r2_score(y, y_c_pcr))\n",
    "    cv_scores.append(r2_score(y, y_cv_pcr))\n",
    "    mse_c.append(mean_squared_error(y, y_c_pcr))\n",
    "    mse_cv.append(mean_squared_error(y, y_cv_pcr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "p = figure(title=\"Erro Quadrático médio\", x_axis_label='Número de componentes', y_axis_label='RMSECV ou RMSEC')\n",
    "    #p.text(x+0.3,y+0.3,flav.index[i])\n",
    "p.line(range(1,len(c_scores)+1),mse_c,legend=\"RMSEC\")\n",
    "p.circle(range(1,len(c_scores)+1),mse_c,legend=\"RMSEC\")\n",
    "p.line(range(1,len(cv_scores)+1),mse_cv,color=\"red\", legend=\"RMSECV\")\n",
    "p.circle(range(1,len(cv_scores)+1),mse_cv,color=\"red\", legend=\"RMSECV\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Erro Validação cruzada\", x_axis_label='Número de componentes', y_axis_label='Q² ou R²')\n",
    "    #p.text(x+0.3,y+0.3,flav.index[i])\n",
    "p.line(range(1,len(c_scores)+1),c_scores,legend=\"R²\")\n",
    "p.circle(range(1,len(c_scores)+1),c_scores,legend=\"R²\")\n",
    "p.line(range(1,len(cv_scores)+1),cv_scores,color=\"red\", legend=\"Q²\")\n",
    "p.circle(range(1,len(cv_scores)+1),cv_scores,color=\"red\", legend=\"Q²\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Tp = T[:,:4]\n",
    "# Calibration\n",
    "pcr.fit(Tp,y)\n",
    "y_c_pcr = pcr.predict(Tp)\n",
    "\n",
    "# Cross-validation\n",
    "y_cv_pcr = cross_val_predict(pcr, Tp, y, cv=len(Xa))\n",
    "print(r2_score(y, y_c_pcr))\n",
    "print(r2_score(y, y_cv_pcr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(mlr.coef_)\n",
    "print(mlr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(pcr.coef_)\n",
    "print(pcr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# PLS\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls2 = PLSRegression(n_components=8)\n",
    "pls2.fit(Xa, y)\n",
    "\n",
    "y_pls = pls2.predict(Xa)\n",
    "print(y_pls)\n",
    "print(pls2.score(Xa,y))\n",
    "\n",
    "y_cv_pls = cross_val_predict(pls2, Xa, y, cv=len(Xa))\n",
    "print(y_cv_pls)\n",
    "print(r2_score(y,y_cv_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cv_pls = cross_val_predict(pls2, Xa, y, cv=len(Xa))\n",
    "print(y_cv_pls)\n",
    "print(r2_score(y,y_cv_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "c_scores = []\n",
    "mse_c = []\n",
    "mse_cv = []\n",
    "for i in range(8):\n",
    "    # Calibration\n",
    "    pls2 = PLSRegression(n_components=i+1)\n",
    "    pls2.fit(Xa,y)\n",
    "    y_c_pcr = pls2.predict(Xa)\n",
    "\n",
    "    # Cross-validation\n",
    "    y_cv_pcr = cross_val_predict(pls2, Xa, y, cv=len(Xa))\n",
    "\n",
    "    c_scores.append(r2_score(y, y_c_pcr))\n",
    "    cv_scores.append(r2_score(y, y_cv_pcr))\n",
    "    mse_c.append(mean_squared_error(y, y_c_pcr))\n",
    "    mse_cv.append(mean_squared_error(y, y_cv_pcr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Erro Quadrático médio\", x_axis_label='Número de varáveis latentes', y_axis_label='RMSECV ou RMSEC')\n",
    "    #p.text(x+0.3,y+0.3,flav.index[i])\n",
    "p.line(range(1,len(c_scores)+1),mse_c,legend=\"RMSEC\")\n",
    "p.circle(range(1,len(c_scores)+1),mse_c,legend=\"RMSEC\")\n",
    "p.line(range(1,len(cv_scores)+1),mse_cv,color=\"red\", legend=\"RMSECV\")\n",
    "p.circle(range(1,len(cv_scores)+1),mse_cv,color=\"red\", legend=\"RMSECV\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import LabelSet, Label, ColumnDataSource\n",
    "source = ColumnDataSource(data=dict(\n",
    "    lv=range(1,len(c_scores)+1),\n",
    "    R2=c_scores,\n",
    "    Q2=cv_scores,\n",
    "))\n",
    "\n",
    "TOOLTIPS = [\n",
    "    (\"LV\", \"@lv\"),\n",
    "    (\"R2\", \"@R2\"),\n",
    "    (\"Q2\", \"@Q2\")\n",
    "]\n",
    "\n",
    "p = figure(title=\"Erro Validação cruzada - PLS\", x_axis_label='Número de variáveis latentes', y_axis_label='Q² ou R²',\n",
    "          tooltips = TOOLTIPS)\n",
    "    #p.text(x+0.3,y+0.3,flav.index[i])\n",
    "p.line(\"lv\",\"R2\",legend=\"R²\", source=source)\n",
    "p.circle(\"lv\",\"R2\",legend=\"R²\", source=source)\n",
    "p.line(\"lv\",\"Q2\",color=\"red\", legend=\"Q²\", source=source)\n",
    "p.circle(\"lv\",\"Q2\",color=\"red\", legend=\"Q²\", source=source)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(Xa, y)\n",
    "\n",
    "y_pls = pls2.predict(Xa)\n",
    "print(pls2.score(Xa,y))\n",
    "\n",
    "y_cv_pls = cross_val_predict(pls2, Xa, y, cv=len(Xa))\n",
    "print(r2_score(y,y_cv_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_a = scaler.transform(X_train)\n",
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(X_train_a, y_train)\n",
    "\n",
    "y_pls = pls2.predict(X_train_a)\n",
    "print(pls2.score(X_train_a,y_train))\n",
    "\n",
    "y_cv_pls = cross_val_predict(pls2, X_train_a, y_train, cv=len(X_train_a))\n",
    "print(r2_score(y_train,y_cv_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test_a = scaler.transform(X_test)\n",
    "y_p = pls2.predict(X_test_a)\n",
    "print(pls2.score(X_test_a,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Z = linkage(Xa,'ward')\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z,orientation=\"right\", labels=[(int(X.index[i]),\"{:.2f}\".format(y[\"pIC50\"][i])) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import LabelSet, Label, ColumnDataSource\n",
    "source = ColumnDataSource(data=dict(x=range(1,1+len(y)),\n",
    "                                    y=y[\"pIC50\"],\n",
    "                                    names=X.index))\n",
    "\n",
    "output_notebook()\n",
    "p = figure(title=\"Atividades\", x_axis_label='Amostras', y_axis_label='pIC50')\n",
    "p.scatter('x','y',color='blue',source=source)\n",
    "labels = LabelSet(x='x', y='y', text='names', level='glyph',\n",
    "              x_offset=5, y_offset=5, source=source, render_mode='canvas')    \n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(Xa)\n",
    "T = pca.fit_transform(Xa)\n",
    "source = ColumnDataSource(data=dict(PC1=T[:,0],\n",
    "                                    PC2=T[:,1],\n",
    "                                    names=X.index))\n",
    "\n",
    "output_notebook()\n",
    "p = figure(title=\"PCA\", x_axis_label='PC1', y_axis_label='PC2')\n",
    "# for i in range(np.shape(T)[0]):\n",
    "#     x = T[i,0]\n",
    "#     y = T[i,1]\n",
    "#     p.circle(x,y,color='blue')\n",
    "    #p.text(x+0.3,y+0.3,flav.index[i])\n",
    "p.scatter('PC1','PC2',color='blue',size=6, source=source)\n",
    "labels = LabelSet(x='PC1', y='PC2', text='names', level='glyph',\n",
    "              x_offset=3, y_offset=3, source=source, render_mode='canvas')    \n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "+ É um método de regressão projetado para tratar dados cujas variáveis que apresentam colinearidade\n",
    "+ Nesse método uma penalização é imposta ao tamanho dos coefcientes de regressão\n",
    "+ A soma de resíduos penalizada é minimizada\n",
    "+ $\\Arrowvert {\\mathbf{y-Xb}} \\Arrowvert^2_{2} + \\alpha\\Arrowvert {\\mathbf{b}} \\Arrowvert^2_{2}= min$\n",
    "+ O parâmetro de complexidade controla a magnitude do \"encolhimento\": quanto maior o valor de $\\alpha$ maior o valor do encolhimento e assim os coeficientes se tornam mais robustos em relação à colinearidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "clfR = RidgeCV(alphas=np.logspace(-3,3,10000))\n",
    "clfR.fit(X_train_a, y_train)\n",
    "clfR.score(X_train_a, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfR.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfR = RidgeCV(alphas=np.linspace(10,40,10000))\n",
    "clfR.fit(X_train_a, y_train)\n",
    "clfR.score(X_train_a, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfR.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cv_Ridge = cross_val_predict(clfR,X_train_a,y_train,cv=len(y_train))\n",
    "r2_score(y_train,y_cv_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfR.predict(X_test_a)\n",
    "clfR.score(X_test_a,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cal_Ridge = clfR.predict(X_train_a)\n",
    "r2_score(y_train,y_cal_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = figure()\n",
    "p.scatter(range(1,565),clfR.coef_.reshape(564))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xsel = X_train_a[:,(np.abs(clfR.coef_) > 0.01)[0]]\n",
    "Xsel_test = X_test_a[:,(np.abs(clfR.coef_) > 0.01)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfR = RidgeCV(alphas=np.logspace(-3,3,10000))\n",
    "clfR.fit(Xsel, y_train)\n",
    "clfR.score(Xsel, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cv_Ridge = cross_val_predict(clfR,Xsel,y_train,cv=len(y_train))\n",
    "r2_score(y_train,y_cv_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfR.predict(Xsel_test)\n",
    "clfR.score(Xsel_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso\n",
    "\n",
    "+ Lasso é um modelo de regressão linear que estima coeficientes esparsos.\n",
    "+ É útil em contextos nos quais coeficientes diferentes de zero são desejados, o que reduz o número de variáveis\n",
    "+ É chamada de uma técnica de regularização L1, pois inclui a norma-1 dos coeficientes na função objetivo a ser minimizada\n",
    "+ $\\frac{1}{2I}\\Arrowvert {\\mathbf{y-Xb}} \\Arrowvert^2_{2} + \\alpha\\Arrowvert {\\mathbf{b}} \\Arrowvert_{1}= min$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "y_train = column_or_1d(y_train)\n",
    "clfLasso = LassoCV(alphas=np.logspace(-3,3,10000),cv = 20)\n",
    "clfLasso.fit(X_train_a, y_train)\n",
    "clfLasso.score(X_train_a, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfLasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cv_Lasso = cross_val_predict(clfLasso,X_train_a,y_train,cv=20)\n",
    "r2_score(y_train,y_cv_Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfLasso.predict(X_test_a)\n",
    "clfLasso.score(X_test_a,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = figure()\n",
    "p.scatter(range(1,565),clfLasso.coef_.reshape(564))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X.columns[clfLasso.coef_ > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elastic-Net\n",
    "\n",
    "+ Elastic-net é um método de regressão linear que usa ambas as normas (L1 e L2) de regularização dos coeficientes\n",
    "+ Essa combinação permite o aprendizado de um modelo esparso no qual poucos coeficientes são diferentes de zero, como o Lasso, enquanto mantém as propriedades de regularização do Ridge\n",
    "+ Elastic-net é útil quando existem múltiplas variáveis correlacionadas entre si. Lasso seleciona uma delas aleatoriamente, enquanto que elastic-net seleciona ambas\n",
    "+ A função objetivo a ser minimizada nesse caso é\n",
    "+ ${ \\frac{1}{2I} ||y-Xb||_2 ^ 2 + \\alpha \\rho ||b||_1 +\\frac{\\alpha(1-\\rho)}{2} ||b||_2 ^ 2} = min$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "clfEnet = LassoCV(cv = 10)\n",
    "clfEnet.fit(X_train_a, y_train)\n",
    "clfEnet.score(X_train_a, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clfEnet.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv_Enet = cross_val_predict(clfEnet,X_train_a,y_train,cv=10)\n",
    "r2_score(y_train,y_cv_Enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfEnet.predict(X_test_a)\n",
    "clfEnet.score(X_test_a,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure()\n",
    "p.scatter(range(1,565),clfEnet.coef_.reshape(564))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[clfEnet.coef_ > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsel2 = X_train_a[:,clfEnet.coef_ > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLSCV(X,y):\n",
    "    cv_scores = []\n",
    "    c_scores = []\n",
    "    mse_c = []\n",
    "    mse_cv = []\n",
    "    for i in range(min(np.shape(X))):\n",
    "        # Calibration\n",
    "        pls2 = PLSRegression(n_components=i+1)\n",
    "        pls2.fit(X,y)\n",
    "        y_c = pls2.predict(X)\n",
    "\n",
    "        # Cross-validation\n",
    "        y_cv = cross_val_predict(pls2, X, y, cv=len(X))\n",
    "\n",
    "    #     c_scores.append(r2_score(y, y_c_pcr))\n",
    "        cv_scores.append(r2_score(y, y_cv))\n",
    "    #     mse_c.append(mean_squared_error(y, y_c_pcr))\n",
    "    #     mse_cv.append(mean_squared_error(y, y_cv_pcr))\n",
    "    return cv_scores\n",
    "cv_scores = PLSCV(Xsel2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(Xsel2, y_train)\n",
    "\n",
    "y_pls = pls2.predict(Xsel2)\n",
    "print(pls2.score(Xsel2,y_train))\n",
    "\n",
    "y_cv_pls = cross_val_predict(pls2, Xsel2, y_train, cv=len(y_train))\n",
    "print(r2_score(y_train,y_cv_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsel2_test = X_test_a[:,clfEnet.coef_ > 0]\n",
    "pls2.predict(Xsel2_test)\n",
    "pls2.score(Xsel2_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tr = GridSearchCV(DecisionTreeRegressor(),{'max_depth':range(1,10)},cv=5)\n",
    "tr.fit(X_train_a,y_train)\n",
    "tr.score(X_train_a,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_predict(tr, Xsel2, y_train, cv=len(y_train))\n",
    "print(r2_score(y_train,cross_val_predict(tr, X_train_a, y_train, cv=len(y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2 = DecisionTreeRegressor(max_depth=5)\n",
    "tr2.fit(X_train_a,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[tr2.feature_importances_ > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos de classificação\n",
    "\n",
    "+ Nos estudos de classificação, cada uma das amostras é descrita por um conjunto de medidas experimentais, que chamamos de \"padrão\" e são classificadas com uma propriedade de interesse (bom/ruim; falso/verdadeiro; ativo/nao ativo etc.)\n",
    "+ A determinação da propriedade de interesse ao atribuir uma amostra à sua respectiva classe é o que chamamos de \"reconhecimento\"\n",
    "+ Na análise supervisionada , seleciona-se uma série de amostras representativas de cada classe e para as quais as medidas experimentais são coletadas e o padrão de cada uma delas é definido (conjunto de treinamento)\n",
    "+ A seguir, utilizando as informações do conjunto de treinamento, construímos um modelo empírico ou regra de classificação, motivo pelo qual esses métodos são chamados de **supervisionados**\n",
    "+ Antes de sua utilização, o modelo empírico obtido deve ser testado para verificar sua capacidade de prever com sucesso a classe de novas amostras. Para isso utiliza-se um conjunto teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos de classificação\n",
    "\n",
    "+ Os métodos de classificação podem ser agrupados em duas categorias:\n",
    "    + Paramétricos ou probabilísticos\n",
    "        + Ex: LDA, QDA, SIMCA etc.\n",
    "    + Não paramétricos ou não probabilísticos\n",
    "        + Ex: k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## k-ésimos vizinhos mais próximos (k-NN)\n",
    "\n",
    "+ Esse é um método conceitualmente bem simples e que produz excelentes resultados\n",
    "+ Durantea construção do modelo, cada amostra do conjunto de treinamento é excluída uma única vez e então classificada usando-se, para isso, as amostras restantes\n",
    "+ São calculadas as distâncias (Euclideana ou Mahalanobis, por exemplo) entre a amostra excluída e todas as outras amostras do conjunto de treinamento\n",
    "+ As distâncias de todas as amostras à amostra em questão são colocadas em ordem crescente para facilitar a identificação dos seus $k$ vizinhos mais próximos\n",
    "+ Essa amostra é então classificada de acordo com a maioria dos \"votos\" de seus vizinhos mais próximos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Conjunto de dados](https://scikit-learn.org/stable/datasets/index.html#wine-recognition-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "vinhos = datasets.load_wine()\n",
    "print(vinhos.data)\n",
    "print(vinhos.target)\n",
    "print(vinhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_vinhos = pd.DataFrame(vinhos.data)\n",
    "df_vinhos.columns = vinhos.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_vinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(vinhos.data,vinhos.target)\n",
    "print(clf.score(vinhos.data,vinhos.target))\n",
    "print(clf.predict_proba(vinhos.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "knn_p = cross_val_predict(clf, vinhos.data, vinhos.target, cv=40)\n",
    "print(knn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "knn = GridSearchCV(clf,{'n_neighbors':range(1,9)},cv=30)\n",
    "knn.fit(vinhos.data,vinhos.target)\n",
    "print(knn.score(vinhos.data,vinhos.target))\n",
    "print(knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(vinhos.data,vinhos.target)\n",
    "print(clf.score(vinhos.data,vinhos.target))\n",
    "print(clf.predict_proba(vinhos.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vinhos.data, vinhos.target, random_state=0)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = vinhos.target_names\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap = pd.read_excel(\"lapachol.xlsx\",index_col=0)\n",
    "lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lap = lap.iloc[:,1:]\n",
    "# y = [cl == \"A\" for cl in lap.loc[:,\"Classe\"]]\n",
    "y_lap = lap.loc[:,\"Classe\"]\n",
    "print(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_lap_a = scaler.fit_transform(X_lap)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knnCV = GridSearchCV(knn,{\"n_neighbors\":range(1,9)},cv=10)\n",
    "knnCV.fit(X_lap_a,y)\n",
    "knnCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [2,8,9,13,15,19,23,24]\n",
    "print(test)\n",
    "train = [i for i in range(1,26) if i not in test]\n",
    "print(train)\n",
    "print(X.index)\n",
    "X_lap_train = X.iloc[train]\n",
    "y_lap_train = [y[i] for i in train]\n",
    "X_lap_test = X.iloc[test]\n",
    "y_lap_test = [y[i] for i in test]\n",
    "X_lap_train_a = scaler.fit_transform(X_lap_train)\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_lap_train_a,y_lap_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lap_pred = knn.predict(scaler.transform(X_lap_test))\n",
    "print(y_pred)\n",
    "class_names_lap = lap[\"Classe\"]\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métricas para avaliar os modelos de classificação (Figuras de mérito)\n",
    "\n",
    "+ Sensibilidade, também conhecida como precisão ou taxa de verdadeiros positivos, é uma medida de quão bem o modelo é capaz de classificar corretamente amostras que realmente apresentam a característica de interesse\n",
    "\n",
    "$SEN = \\frac{VP}{VP+FN} \\times 100\\%$\n",
    "\n",
    "+ Especificidade ou seletividade nos informa a habilidade do modelo em identificar corretamente amostras que não são da classe de interesse\n",
    "\n",
    "$SEL = \\frac{VN}{FP+VN} \\times 100\\%$\n",
    "\n",
    "+ Exatidão fornece uma ideia geral do seu desempenho, considerando o percentual total de acertos\n",
    "\n",
    "$S = \\frac{VP+VN}{VP+FN+FP+VN} \\times 100\\% = 100 - (SEN+SEL)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise Discriminante Linear (LDA)\n",
    "\n",
    "+ Na análise discriminante linear (LDA, do inglês *linear discriminant analysis*), deseja-se definir uma superfície de decisão de modo que as amostras de uma classe estejam de um lado do hiperplano e as amostras da outra classe estejam do outro lado\n",
    "+ A superfície de decisão ou classificador é o hiperplano que satisfaz à seguinte equação\n",
    "\n",
    "$k(x) = a_0 + a_1x_1 + a_2x_2 + a_3x_3 + \\cdots = 0$\n",
    "\n",
    "+ As amostras de uma classe, que ficariam acima do hiperplano, devem ter coordenadas de modo que $k(x) > 0$, enquanto que as amostras da outra classe devem ter $k(x) <0$\n",
    "+ Os valores dos coeficientes $a_j$ podem ser obtidos usando MLR tendo como variável dependente valores binários para discriminar as amostras entre as classes (1 e 0 ou +1 e -1 por exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(vinhos.data,vinhos.target)\n",
    "print(lda.score(vinhos.data,vinhos.target))\n",
    "print(lda.predict_proba(vinhos.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lda.fit(X_train, y_train).predict(X_test)\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lda.fit(scaler.fit_transform(X_lap_train), y_lap_train)\n",
    "y_lap_pred = lda.predict(scaler.transform(X_lap_test))\n",
    "print(y_lap_pred)\n",
    "class_names_lap = lap[\"Classe\"]\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Comparação entre LDA e PCA](plot_pca_vs_lda.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análise Discriminante Quadrática (QDA)\n",
    "\n",
    "+ Na análise discriminante quadrática (QDA, do inglês *quadratic discriminant analysis*), utiliza-se um modelo quadrático para discriminar as amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(vinhos.data,vinhos.target)\n",
    "print(qda.score(vinhos.data,vinhos.target))\n",
    "print(qda.predict_proba(vinhos.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = qda.fit(X_train, y_train).predict(X_test)\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qda.fit(scaler.fit_transform(X_lap_train), y_lap_train)\n",
    "y_lap_pred = qda.predict(scaler.transform(X_lap_test))\n",
    "print(y_lap_pred)\n",
    "class_names_lap = lap[\"Classe\"]\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Comparação entre LDA e QDA](plot_lda_qda.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Máquinas de suporte vetorial (SVM)\n",
    "\n",
    "+ Uma máquina de suporte vetorial (SVM, do inglês Support Vector Machine), constrói um hiperplano ou conjunto de hiperplanos em um espaço de maior dmensão ou dimensão infinita, que pode ser usado para classificação ou regressão\n",
    "+ Intuitivamente, uma boa separação é alcançada pelo hiperplano que tem a maior distância para os vizinhos mais próximos de cada classe no conjunto de treinamento (distância conhecida como **margem**)\n",
    "\n",
    "![svm](svm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma=\"scale\")\n",
    "svc.fit(vinhos.data,vinhos.target)\n",
    "print(svc.score(vinhos.data,vinhos.target))\n",
    "print(svc.predict(vinhos.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.fit(X_train, y_train).predict(X_test)\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(vinhos.data,vinhos.target)\n",
    "print(clf.score(vinhos.data,vinhos.target))\n",
    "print(clf.predict(vinhos.data))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(scaler.fit_transform(X_lap_train), y_lap_train)\n",
    "y_lap_pred = qda.predict(scaler.transform(X_lap_test))\n",
    "print(y_lap_pred)\n",
    "class_names_lap = lap[\"Classe\"]\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_lap_test, y_lap_pred, classes=class_names_lap, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"linear\",C=1)\n",
    "svc.fit(scaler.fit_transform(X_lap_train),y_lap_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit_transform(X_lap_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
